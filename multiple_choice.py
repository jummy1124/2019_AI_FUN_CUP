# -*- coding: utf-8 -*-
"""Multiple_choice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyYN-avUvnQTiPY_yD1QJDG4KI4imNDW
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

path='//content//drive//My Drive//FUN CUP測試資料100題//'

pip install SpeechRecognition

!pip install https://github.com/huggingface/transformers/releases/download/v0.1.2/pytorch_pretrained_bert-0.1.2.tar.gz

!nvidia-smi
!rm -rf bert_chinese*
!rm -rf pytorch_pretrained_bert*
!wget -q --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1SXz4Hr073OCuIOqzmi1Ool6m9-g8yYOf' -O bert_chinese.zip
!unzip -q -o bert_chinese.zip
!wget -q --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=15vkVWzKDlik_l2MAkvGI6tIhPauQiGaW' -O pytorch_pretrained_bert.zip
!unzip -q -o pytorch_pretrained_bert.zip
# !wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1IIVjgGX_JeXBNQv-vRAj1JLFvv2x9EVu' -O pytorch_pretrained_bert-0.1.2.zip
# !unzip pytorch_pretrained_bert-0.1.2.zip
# %cd pytorch_pretrained_bert-0.1.2
# !python setup.py install
# %cd ..

import torch
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
                              TensorDataset)
import numpy as np
from tqdm import tqdm


def to_list(tensor):
    return tensor.detach().cpu().tolist()
 

def _get_best_indexes(logits, n_best_size=1):
    """Get the n-best logits from a list."""
    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)

    best_indexes = []
    for i in range(len(index_and_score)):
        if i >= n_best_size:
            break
        best_indexes.append(index_and_score[i][0])
    return best_indexes
 
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=1, keepdims=True)
    
def predict(eval_dataloader, model, tokenizer, device='cpu'):


    model.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions = []
    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc='Iteration'):
        input_ids = input_ids.to(device)
        input_mask = input_mask.to(device)
        segment_ids = segment_ids.to(device)
        label_ids = label_ids.to(device)

        
        with torch.no_grad():
            # print(input_ids.shape)
#             tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)
            logits = model(input_ids, segment_ids, input_mask, label_ids)

        logits = logits.detach().cpu().numpy()
        print("logits:",logits)
        label_ids = label_ids.to('cpu').numpy()
        
        logits = softmax(logits.tolist())
        for logit in logits:
            predictions.append(logit)

    # return np.argmax(predictions)
    return np.argmax(predictions[0])

import collections

from torch.utils.data import TensorDataset, DataLoader, SequentialSampler


class InputFeatures(object):
    """A single set of features of data."""

    def __init__(self, input_ids, input_mask, segment_ids, label_id):
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.segment_ids = segment_ids
        self.label_id = label_id

        
def _truncate_seq_pair(tokens_a, tokens_b, max_length):
    """Truncates a sequence pair in place to the maximum length."""

    # This is a simple heuristic which will always truncate the longer sequence
    # one token at a time. This makes more sense than truncating an equal percent
    # of tokens from each, since if one sequence is very short then each token
    # that's truncated likely contains more information than a longer sequence.
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_length:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()
            
            
def convert_examples_to_features(context, question, choices, tokenizer, ans, max_seq_length=300, label_list=[1,2,3,4]):
    
    text_bs = list()
    for choice in choices:
        # text_b = tokenization.convert_to_unicode(str(row['choice%d' % i]))
        text_bs.append("{} {}".format(question, choice))
        #text_b.append(choice)
      

    label_map = {}
    for (i, label) in enumerate(label_list):
        label_map[label] = i    

    multiple_choice_input_ids = []
    multiple_choice_input_masks = []
    multiple_choice_segment_ids = []
    for text_b in text_bs:  
    

        tokens_a = tokenizer.tokenize(context)  

        tokens_b = None
        if text_b:
            tokens_b = tokenizer.tokenize(text_b)   

        if tokens_b:
            # Modifies `tokens_a` and `tokens_b` in place so that the total
            # length is less than the specified length.
            # Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            # Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) > max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)] 

        
        tokens = []
        segment_ids = []
        tokens.append("[CLS]")
        segment_ids.append(0)
        for token in tokens_a:
            tokens.append(token)
            segment_ids.append(0)
        tokens.append("[SEP]")
        segment_ids.append(0)   

        if tokens_b:
            for token in tokens_b:
                tokens.append(token)
                segment_ids.append(1)
            tokens.append("[SEP]")
            segment_ids.append(1)   

        input_ids = tokenizer.convert_tokens_to_ids(tokens) 

        # The mask has 1 for real tokens and 0 for padding tokens. Only real
        # tokens are attended to.
        input_mask = [1] * len(input_ids)   

        # Zero-pad up to the sequence length.
        while len(input_ids) < max_seq_length:
            input_ids.append(0)
            input_mask.append(0)
            segment_ids.append(0)   

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(segment_ids) == max_seq_length
        multiple_choice_input_ids.append(input_ids)
        multiple_choice_input_masks.append(input_mask)
        multiple_choice_segment_ids.append(segment_ids)
    assert len(multiple_choice_input_ids) == 4
    assert len(multiple_choice_input_masks) == 4
    assert len(multiple_choice_segment_ids) == 4    

    label_id = label_map[ans] 
    
    

    f = InputFeatures(input_ids=multiple_choice_input_ids,
                      input_mask=multiple_choice_input_masks,
                      segment_ids=multiple_choice_segment_ids,
                      label_id=label_id)  
    
    

    all_input_ids = torch.tensor([f.input_ids], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id], dtype=torch.long)
    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)
    eval_dataloader = DataLoader(eval_data, sampler=SequentialSampler(eval_data), batch_size=1)

    return eval_dataloader, tokens

# 因為要我們今天要跑的是中文QA 所以只有Bert可以用
import torch
from pytorch_pretrained_bert import (BertConfig, BertForSequenceClassification,
                                 BertTokenizer)

device = torch.device("cuda")
 
model_name = 'bert_chinese'
config_class, tokenizer_class = BertConfig, BertTokenizer
# model = model_class.from_pretrained('bert_chinese').to(device)
model = torch.load(model_name + '/model.cpt') 
tokenizer = torch.load(model_name + '/tokenizer.cpt')

import pandas as pd

test_df=pd.read_csv('/content/drive/My Drive/test_choice_4.csv')
test_df.head()

ans=list()
for i in range(len(test_df)):
  context = test_df['文章'][i]
  question = test_df['題目'][i]
  choices = [test_df['選項一'][i], test_df['選項二'][i], test_df['選項三'][i], test_df['選項四'][i]]
  data, tokens = convert_examples_to_features(context, question, choices, tokenizer, 2)
  pred = predict(data, model, tokenizer, device=device)
  print(data)
  # print('Predicted answer: ', choices[pred])
  ans.append(pred+1)

ans=pd.DataFrame(ans)
ans.to_csv('predict.csv',index=False)